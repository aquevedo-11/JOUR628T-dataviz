geom_bar(aes(x=year, weight=value)) +
labs(
title="Annual Port of New York-New Jersey TEU Volumes 2018-2022",
x = "Year",
y = "Total TEUs",
caption = "Source: Port Authority of New York and New Jersey")
new_port_of_va <- port_of_va |> rename(count_type = x1) |> rename(load_type = x2) |> rename(`2018` = x2018) |> rename(`2019` = x2019) |> rename(`2020` = x2020) |> rename(`2021` = x2021) |> rename(`2022` = x2022)
port_of_va_gg <- new_port_of_va |>
select(count_type, load_type, `2018`, `2019`, `2020`, `2021`, `2022`) |>
filter(count_type == "Total") |> filter(load_type == "Total TEUs") |>
#pivot_longer(cols = -count_type, names_to = "year", values_to = "value")
pivot_longer(cols = -c(count_type, load_type), names_to = "year", values_to = "value")
port_of_va_gg |>
ggplot() +
geom_bar(aes(x=year, weight=value)) +
labs(
title="Annual Port of Virginia TEU Volumes 2019-2022",
x = "Year",
y = "Total TEUs",
caption = "Source: Port of Virginia")
port_of_bmore_gg <- port_of_bmore |>
mutate(month = my(month)) |>
rename(date = month) |>
mutate(year = format(as.Date(date), "%Y")) |>
select(year, total_loaded_te_us) |>
filter(year == "2018" | year == "2019" | year == "2020" | year == "2021" | year == "2022") |>
group_by(year) |>
summarise(value = sum(total_loaded_te_us))
port_of_bmore_gg |>
ggplot() +
geom_bar(aes(x=year, weight=value)) +
labs(
title="Annual Port of Baltimore TEU Volumes 2019-2022",
x = "Year",
y = "Total TEUs",
caption = "Source: Maryland Port Administration")
final_3_gg <- inner_join(port_of_bmore_gg, port_of_nynj_gg, by = "year") %>%
inner_join(port_of_va_gg, by = "year") |>
rename(Baltimore = value.x) |> rename(NY_NJ = value.y) |> rename(VA = value) |>
select(year, Baltimore, NY_NJ, VA )
restructured_final_3 <- final_3_gg |>
pivot_longer(cols = -year, names_to = "port", values_to = "value") |>
arrange(year)
ggplot() +
geom_bar(data=restructured_final_3, aes(x = year, weight=value, fill = port)) +
labs(title = "Port Volumes Over Years",
x = "Year",
y = "Totals",
caption = "Source USDOT, Port Authority of New York and New Jersey, Maryland Port Administration, Port of Virginia")
#coord_flip()
#geom_line(aes(x = year, y = VA, color = "Virginia")) +
#geom_line(aes(x = year, y = NY_NJ, color = "NY_NJ")) +
#geom_line(aes(x = year, y = Baltimore, color = "Baltimore")) +
#labs(title = "Port Volumes Over Years",
#     x = "year",
#     y = "value") +
#scale_x_continuous(breaks = seq(2018, 2022, by = 1)) +
#scale_color_manual(name = "port", values = c("Virginia" = "blue", "NY_NJ" = "yellow", "Baltimore" = "orange")) +
#theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(dplyr)
library(ggplot2)
stand_dests_df <- read_csv("archive/standardize_sponsors_dests/standardizing_dests/house_12to23_w_stand_dests.csv")
stand_dests_df
sponsor_list <- read_delim("archive/standardize_sponsors_dests/standardized_sponsors/open_refined_sponsors.csv")
sponsor_list
house_travel <- stand_dests_df %>%
full_join(sponsor_list)
# Convert "ReturnDate" to Date type
house_travel$ReturnDate <- as.Date(house_travel$ReturnDate, format = "%m/%d/%Y")
# Extract the year from "DepartureDate" and populate "Year" column
house_travel$Year <- year(house_travel$DepartureDate)
# Extract the year/month combination from "DepartureDate" and populate "Year" column
house_travel$year_month <- paste(year(house_travel$DepartureDate), month(house_travel$DepartureDate, label = TRUE), sep = "-")
# Filter out non-members or staffers
house_travel <- house_travel %>%
filter(!is.na(State) & !is.na(District))
# Print or use house_travel as needed
print(house_travel)
# Identify all pairs in the house_travel dataset that have matching DocIDs and save those with matching DocIDs for further review.
duplicate_docids_house_travel <- house_travel %>%
group_by(DocID) %>%
filter(n() > 1)
combined_duplicate_docids <- duplicate_docids_house_travel %>%
group_by(DocID) %>%
summarise(
across(
-c(DepartureDate, ReturnDate),
~ ifelse(
length(unique(na.omit(.))) > 1,
paste(na.omit(.), collapse = "; "),
first(na.omit(.))
)
),
DepartureDate = min(DepartureDate, na.rm = TRUE), #This is an imperfect method of combining travel date information, but because these forms have identical date information, it works.
ReturnDate = max(ReturnDate, na.rm = TRUE)
)
filtered_house_travel <- house_travel %>%
anti_join(duplicate_docids_house_travel, by = "DocID")
# Identify duplicate rows based on the combination of values in specified columns that remain consistent from form to form.
# To cast a wide net, we create one version that identifies rows with matching values in the FilerName, MemberName, State, Year and standardized_dest columns -- columns that consistently match from entry to entry.
columns_wide <- c("FilerName", "MemberName", "State", "Year", "standardized_dest")
# To cast a narrower net, we create another version that identifies rows with matching values in the FilerName, MemberName, State, and standardized_dest columns, as well as the year_month and sponsors_cleaned columns -- columns that may change within an otherwise matching set.
columns_narrow <- c("FilerName", "MemberName", "State", "year_month", "standardized_dest", "sponsors_cleaned")
duplicates_logical_wide <- duplicated(filtered_house_travel[columns_wide], fromLast = TRUE) | duplicated(filtered_house_travel[columns_wide])
duplicates_logical_narrow <- duplicated(filtered_house_travel[columns_narrow], fromLast = TRUE) | duplicated(filtered_house_travel[columns_narrow])
# Subset the original dataset using the wide and narrow duplicate vectors to get the duplicate rows
duplicates_wide <- subset(filtered_house_travel, duplicates_logical_wide)
duplicates_narrow <- subset(filtered_house_travel, duplicates_logical_narrow)
# Arrange the rows to consolidate matching groups.
duplicates_wide <- arrange(duplicates_wide, FilerName, MemberName, State, Year, standardized_dest)
duplicates_narrow <- arrange(duplicates_narrow, FilerName, MemberName, State, Year, standardized_dest, sponsors_cleaned)
missing <- anti_join(duplicates_narrow, duplicates_wide, by = c("DocID"))
# Use the DocID column to identify the most recent version of a travel filing.
rows_to_keep <- duplicates_narrow %>%
group_by(FilerName, MemberName, State, year_month, standardized_dest, sponsors_cleaned) %>%
filter(DocID == max(DocID))
diff_sponsor_month <- anti_join(duplicates_wide, duplicates_narrow, by = c("FilerName", "MemberName", "State", "year_month", "standardized_dest", "sponsors_cleaned"))
no_change_diff_sponsor_month_rows <- diff_sponsor_month %>%
group_by(across(-c(TravelSponsor, sponsors_cleaned, DocID, index_number, FilingType, Destination))) %>%
filter(n() == 1) %>%
ungroup()
consolidated_diff_sponsor_rows <- diff_sponsor_month %>%
group_by(across(-c(TravelSponsor, sponsors_cleaned, DocID, index_number, FilingType, Destination))) %>%
filter(n() > 1) %>%
slice_max(order_by = DocID) %>%
ungroup()
filtered_house_travel <- anti_join(filtered_house_travel, duplicates_wide, by = c("DocID"))
# Convert 'index_number' to numeric in combined_duplicate_docids
combined_duplicate_docids <- mutate(combined_duplicate_docids, index_number = as.numeric(index_number))
# Now you can bind the rows
filtered_house_travel <- bind_rows(filtered_house_travel, rows_to_keep)
filtered_house_travel <- bind_rows(filtered_house_travel, combined_duplicate_docids)
filtered_house_travel <- bind_rows(filtered_house_travel, consolidated_diff_sponsor_rows)
# Group by the specified columns and count the occurrences of each combination
grouped_same_dest_diff_spons <- no_change_diff_sponsor_month_rows %>%
group_by(MemberName, FilerName, State, standardized_dest, year_month) %>%
summarise(count = n()) %>%
filter(count > 1)
# Filter for rows where count is greater than 1, meaning they have duplicates
same_dest_diff_spons <- no_change_diff_sponsor_month_rows %>%
inner_join(grouped_same_dest_diff_spons, by = c('MemberName', 'FilerName', 'State', 'standardized_dest', 'year_month'))
# Group by the specified columns and count the occurrences of each combination
grouped_same_dest_diff_month <- no_change_diff_sponsor_month_rows %>%
group_by(MemberName, FilerName, State, standardized_dest, sponsors_cleaned) %>%
summarise(count = n()) %>%
filter(count > 1)
# Filter for rows where count is greater than 1, meaning they have duplicates
same_dest_diff_month <- no_change_diff_sponsor_month_rows %>%
inner_join(grouped_same_dest_diff_month, by = c('MemberName', 'FilerName', 'State', 'standardized_dest', 'sponsors_cleaned'))
no_change_diff_sponsor_month_rows <- subset(no_change_diff_sponsor_month_rows,
!(MemberName == "Beyer, Donald" &
FilingType == "Original" &
DepartureDate == "2021-11-19" &
sponsors_cleaned == "United Kingdom Marshall Scholars"))
possible_duplicates <- subset(no_change_diff_sponsor_month_rows,
sponsors_cleaned %in% c("Us-qatar Business Council",
"Hillsdale College",
"Washington Office On Latin America",
"Japan Center For International Exchange"))
filtered_no_change_diff_sponsor_month_rows <- anti_join(no_change_diff_sponsor_month_rows, possible_duplicates, by = c("DocID"))
filtered_house_travel <- bind_rows(filtered_house_travel, filtered_no_change_diff_sponsor_month_rows)
# Write the filtered house travel dataset to a CSV file
write_csv(filtered_house_travel, "deduped_house_travel_full.csv")
# Write the remaining possible duplicates dataset to a CSV file.
write_csv(possible_duplicates, "possible_duplicates.csv")
filtered_house_travel %>%
group_by(sponsors_cleaned) %>%
summarise(count = n()) %>%
arrange(desc(count))
sugar_filtered <- filtered_house_travel[grepl("Sugar|Sugarbeet|Sugarcane|South Florida Agricultural|Agricultural Institute of Florida|Louisiana Farm Bureau|Leadership Idaho Agriculture", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
sugar_filtered |>
arrange(desc(DepartureDate))
sugar_filtered |>
group_by(sponsors_cleaned) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
group_by(MemberName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
group_by(State) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
# Convert the "Date" column to Date type if it's not already
sugar_filtered$Date <- as.Date(sugar_filtered$DepartureDate)
# Create a line chart
sugar_filtered_plot <- ggplot(sugar_filtered, aes(x = DepartureDate)) +
geom_line(stat = "count", color = "blue") +
labs(title = "Number of Trips Sponsored in Whole or in Part by Sugar Interests",
x = "Date",
y = "Trips") +
theme_minimal()
sugar_filtered_plot
sugar_filtered <- sugar_filtered %>%
mutate(Month = format(DepartureDate, "%Y-%m"))
sugar_dests_by_month_filtered <- sugar_filtered %>%
group_by(Month, standardized_dest) %>%
summarise(count = n()) %>%
arrange(desc(count))
top5_per_month_sugar_filtered <- sugar_dests_by_month_filtered %>%
group_by(Month) %>%
top_n(5, count)
top5_per_month_sugar_filtered |>
arrange(desc(Month))
# Filter rows to isolate trips sponsored in whole or in part by other agricultural interests.
other_ag <- filtered_house_travel[grepl("St. Louis Agribusiness Club|Farm Journal Foundation|National Association of State Departments of Agriculture|Farm Credit of the Virginias|Bowery Farming|Massachusetts Farm Bureau|National Farmers Union|National Grain and Feed Association|North Carolina Vegetable Growers Association|Livestock Marketing Association|Farm Foundation", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
other_ag|>
arrange(desc(DepartureDate))
# Filter rows to isolate trips sponsored by conservation or clean energy organizations that may have an interest in the sugar industry or its environmental impacts.
conservation_sugar_regions_narrow <- house_travel[grepl("The Everglades Foundation|National Parks Conservation Association|Center for Clean Air Policy|Coalition to Restore Coastal Louisiana|Idaho Conservation League", house_travel$sponsors_cleaned, ignore.case = TRUE), ]
conservation_sugar_regions_narrow|>
arrange(desc(DepartureDate))
louisiana_sugar <- subset(sugar_filtered, grepl("Louisiana Sugar", TravelSponsor, ignore.case = TRUE))
# Display the resulting subset
print(louisiana_sugar)
louisiana_sugar |>
group_by(Year) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
louisiana_sugar |>
group_by(MemberName) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
midwest_sugar <- sugar_filtered[grepl("Sugarbeet|Leadership Idaho Agriculture", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
View(midwest_sugar)
View(midwest_sugar)
midwest_sugar <- sugar_filtered[grepl("Sugarbeet|Leadership Idaho Agriculture}|Sugar", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
View(midwest_sugar)
View(midwest_sugar)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(dplyr)
library(ggplot2)
stand_dests_df <- read_csv("archive/standardize_sponsors_dests/standardizing_dests/house_12to23_w_stand_dests.csv")
stand_dests_df
sponsor_list <- read_delim("archive/standardize_sponsors_dests/standardized_sponsors/open_refined_sponsors.csv")
sponsor_list
house_travel <- stand_dests_df %>%
full_join(sponsor_list)
# Convert "ReturnDate" to Date type
house_travel$ReturnDate <- as.Date(house_travel$ReturnDate, format = "%m/%d/%Y")
# Extract the year from "DepartureDate" and populate "Year" column
house_travel$Year <- year(house_travel$DepartureDate)
# Extract the year/month combination from "DepartureDate" and populate "Year" column
house_travel$year_month <- paste(year(house_travel$DepartureDate), month(house_travel$DepartureDate, label = TRUE), sep = "-")
# Filter out non-members or staffers
house_travel <- house_travel %>%
filter(!is.na(State) & !is.na(District))
# Print or use house_travel as needed
print(house_travel)
# Identify all pairs in the house_travel dataset that have matching DocIDs and save those with matching DocIDs for further review.
duplicate_docids_house_travel <- house_travel %>%
group_by(DocID) %>%
filter(n() > 1)
combined_duplicate_docids <- duplicate_docids_house_travel %>%
group_by(DocID) %>%
summarise(
across(
-c(DepartureDate, ReturnDate),
~ ifelse(
length(unique(na.omit(.))) > 1,
paste(na.omit(.), collapse = "; "),
first(na.omit(.))
)
),
DepartureDate = min(DepartureDate, na.rm = TRUE), #This is an imperfect method of combining travel date information, but because these forms have identical date information, it works.
ReturnDate = max(ReturnDate, na.rm = TRUE)
)
filtered_house_travel <- house_travel %>%
anti_join(duplicate_docids_house_travel, by = "DocID")
# Identify duplicate rows based on the combination of values in specified columns that remain consistent from form to form.
# To cast a wide net, we create one version that identifies rows with matching values in the FilerName, MemberName, State, Year and standardized_dest columns -- columns that consistently match from entry to entry.
columns_wide <- c("FilerName", "MemberName", "State", "Year", "standardized_dest")
# To cast a narrower net, we create another version that identifies rows with matching values in the FilerName, MemberName, State, and standardized_dest columns, as well as the year_month and sponsors_cleaned columns -- columns that may change within an otherwise matching set.
columns_narrow <- c("FilerName", "MemberName", "State", "year_month", "standardized_dest", "sponsors_cleaned")
duplicates_logical_wide <- duplicated(filtered_house_travel[columns_wide], fromLast = TRUE) | duplicated(filtered_house_travel[columns_wide])
duplicates_logical_narrow <- duplicated(filtered_house_travel[columns_narrow], fromLast = TRUE) | duplicated(filtered_house_travel[columns_narrow])
# Subset the original dataset using the wide and narrow duplicate vectors to get the duplicate rows
duplicates_wide <- subset(filtered_house_travel, duplicates_logical_wide)
duplicates_narrow <- subset(filtered_house_travel, duplicates_logical_narrow)
# Arrange the rows to consolidate matching groups.
duplicates_wide <- arrange(duplicates_wide, FilerName, MemberName, State, Year, standardized_dest)
duplicates_narrow <- arrange(duplicates_narrow, FilerName, MemberName, State, Year, standardized_dest, sponsors_cleaned)
missing <- anti_join(duplicates_narrow, duplicates_wide, by = c("DocID"))
# Use the DocID column to identify the most recent version of a travel filing.
rows_to_keep <- duplicates_narrow %>%
group_by(FilerName, MemberName, State, year_month, standardized_dest, sponsors_cleaned) %>%
filter(DocID == max(DocID))
diff_sponsor_month <- anti_join(duplicates_wide, duplicates_narrow, by = c("FilerName", "MemberName", "State", "year_month", "standardized_dest", "sponsors_cleaned"))
no_change_diff_sponsor_month_rows <- diff_sponsor_month %>%
group_by(across(-c(TravelSponsor, sponsors_cleaned, DocID, index_number, FilingType, Destination))) %>%
filter(n() == 1) %>%
ungroup()
consolidated_diff_sponsor_rows <- diff_sponsor_month %>%
group_by(across(-c(TravelSponsor, sponsors_cleaned, DocID, index_number, FilingType, Destination))) %>%
filter(n() > 1) %>%
slice_max(order_by = DocID) %>%
ungroup()
filtered_house_travel <- anti_join(filtered_house_travel, duplicates_wide, by = c("DocID"))
# Convert 'index_number' to numeric in combined_duplicate_docids
combined_duplicate_docids <- mutate(combined_duplicate_docids, index_number = as.numeric(index_number))
# Now you can bind the rows
filtered_house_travel <- bind_rows(filtered_house_travel, rows_to_keep)
filtered_house_travel <- bind_rows(filtered_house_travel, combined_duplicate_docids)
filtered_house_travel <- bind_rows(filtered_house_travel, consolidated_diff_sponsor_rows)
# Group by the specified columns and count the occurrences of each combination
grouped_same_dest_diff_spons <- no_change_diff_sponsor_month_rows %>%
group_by(MemberName, FilerName, State, standardized_dest, year_month) %>%
summarise(count = n()) %>%
filter(count > 1)
# Filter for rows where count is greater than 1, meaning they have duplicates
same_dest_diff_spons <- no_change_diff_sponsor_month_rows %>%
inner_join(grouped_same_dest_diff_spons, by = c('MemberName', 'FilerName', 'State', 'standardized_dest', 'year_month'))
# Group by the specified columns and count the occurrences of each combination
grouped_same_dest_diff_month <- no_change_diff_sponsor_month_rows %>%
group_by(MemberName, FilerName, State, standardized_dest, sponsors_cleaned) %>%
summarise(count = n()) %>%
filter(count > 1)
# Filter for rows where count is greater than 1, meaning they have duplicates
same_dest_diff_month <- no_change_diff_sponsor_month_rows %>%
inner_join(grouped_same_dest_diff_month, by = c('MemberName', 'FilerName', 'State', 'standardized_dest', 'sponsors_cleaned'))
no_change_diff_sponsor_month_rows <- subset(no_change_diff_sponsor_month_rows,
!(MemberName == "Beyer, Donald" &
FilingType == "Original" &
DepartureDate == "2021-11-19" &
sponsors_cleaned == "United Kingdom Marshall Scholars"))
possible_duplicates <- subset(no_change_diff_sponsor_month_rows,
sponsors_cleaned %in% c("Us-qatar Business Council",
"Hillsdale College",
"Washington Office On Latin America",
"Japan Center For International Exchange"))
filtered_no_change_diff_sponsor_month_rows <- anti_join(no_change_diff_sponsor_month_rows, possible_duplicates, by = c("DocID"))
filtered_house_travel <- bind_rows(filtered_house_travel, filtered_no_change_diff_sponsor_month_rows)
# Write the filtered house travel dataset to a CSV file
write_csv(filtered_house_travel, "deduped_house_travel_full.csv")
# Write the remaining possible duplicates dataset to a CSV file.
write_csv(possible_duplicates, "possible_duplicates.csv")
filtered_house_travel %>%
group_by(sponsors_cleaned) %>%
summarise(count = n()) %>%
arrange(desc(count))
sugar_filtered <- filtered_house_travel[grepl("Sugar|Sugarbeet|Sugarcane|South Florida Agricultural|Agricultural Institute of Florida|Louisiana Farm Bureau|Leadership Idaho Agriculture", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
sugar_filtered |>
arrange(desc(DepartureDate))
sugar_filtered |>
group_by(sponsors_cleaned) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
group_by(MemberName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
group_by(State) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
# Convert the "Date" column to Date type if it's not already
sugar_filtered$Date <- as.Date(sugar_filtered$DepartureDate)
# Create a line chart
sugar_filtered_plot <- ggplot(sugar_filtered, aes(x = DepartureDate)) +
geom_line(stat = "count", color = "blue") +
labs(title = "Number of Trips Sponsored in Whole or in Part by Sugar Interests",
x = "Date",
y = "Trips") +
theme_minimal()
sugar_filtered_plot
sugar_filtered <- sugar_filtered %>%
mutate(Month = format(DepartureDate, "%Y-%m"))
sugar_dests_by_month_filtered <- sugar_filtered %>%
group_by(Month, standardized_dest) %>%
summarise(count = n()) %>%
arrange(desc(count))
top5_per_month_sugar_filtered <- sugar_dests_by_month_filtered %>%
group_by(Month) %>%
top_n(5, count)
top5_per_month_sugar_filtered |>
arrange(desc(Month))
# Filter rows to isolate trips sponsored in whole or in part by other agricultural interests.
other_ag <- filtered_house_travel[grepl("St. Louis Agribusiness Club|Farm Journal Foundation|National Association of State Departments of Agriculture|Farm Credit of the Virginias|Bowery Farming|Massachusetts Farm Bureau|National Farmers Union|National Grain and Feed Association|North Carolina Vegetable Growers Association|Livestock Marketing Association|Farm Foundation", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
other_ag|>
arrange(desc(DepartureDate))
# Filter rows to isolate trips sponsored by conservation or clean energy organizations that may have an interest in the sugar industry or its environmental impacts.
conservation_sugar_regions_narrow <- house_travel[grepl("The Everglades Foundation|National Parks Conservation Association|Center for Clean Air Policy|Coalition to Restore Coastal Louisiana|Idaho Conservation League", house_travel$sponsors_cleaned, ignore.case = TRUE), ]
conservation_sugar_regions_narrow|>
arrange(desc(DepartureDate))
louisiana_sugar <- subset(sugar_filtered, grepl("Louisiana Sugar", TravelSponsor, ignore.case = TRUE))
# Display the resulting subset
print(louisiana_sugar)
louisiana_sugar |>
group_by(Year) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
louisiana_sugar |>
group_by(MemberName) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
midwest_sugar <- sugar_filtered[grepl("Sugarbeet|Leadership Idaho Agriculture", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
florida_sugar <- sugar_filtered[grepl("Florida", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
ag_inst_florida_sugar <- sugar_filtered[grepl("Agricultural Institute Of Florida Foundation", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
View(conservation_sugar_regions_narrow)
View(sugar_filtered)
knitr::opts_chunk$set(echo = TRUE)
ggplot(mcr_area, aes(area = count_by_state, fill = State)) +
geom_treemap() +
theme_minimal() +
geom_treemap_text(grow = TRUE, reflow = TRUE,
aes(label = sprintf("%s\n(%s)", State, format(count_by_state, big.mark = ","))),
colour = "white", place = "bottomright", size = 1) +
scale_fill_manual(values = c("CA" = "#060102", "TX" = "#2e0a0f", "NY" = "#861d2c", "FL" = "#491018", "CO" = "#5d141f", "CT" = "#711825", "GA" = "#861d2c", "MA" = "#9a2133", "MI" = "#ae2539", "MN" = "#c22a40", "NC" = "#d23249", "NJ" = "#d6465b", "NV" = "#d95467", "OH" = "#df6e7f", "OK" = "#e48291", "OR" = "#eba4ae", "PA" = "#efb8c0", "TN" = "#f4ccd2", "WA" = "#f6dade" )) +
labs(title = "Black Parade Stops", subtitle = "Stops along the U.S. leg of the MCR Reunion tour", caption = "Source: Touring Data \n by April Quevedo", color = "State")
mcr_vampiremoney <- read.csv("mcr_reunion_tour_r.csv")
#  group_by(State) |> summarize(Revenue)
#Create data frame
mcr_vampiremoney <- tibble(
State = c("CA", "CA", "CO", "CT", "FL", "GA", "MA", "MI", "MN", "NC",
"NJ", "NV", "NY", "NY", "NY", "NY", "OH", "OK", "OR", "PA",
"TN", "TX", "TX", "TX", "WA"),
Revenue = c("$2,312,402.00", "$9,379,280.00", "$2,303,500.00", "$947,219.00",
"$2,186,088.00", "$1,368,605.00", "$3,875,419.00", "$2,570,993.00",
"$2,323,892.00", "$2,482,991.00", "$3,756,499.00", "$2,671,341.00",
"$1,364,591.00", "$997,090.00", "$1,445,376.00", "$2,955,027.00",
"$1,652,303.00", "$1,963,336.00", "$1,776,165.00", "$2,769,425.00",
"$2,057,097.00", "$2,087,747.00", "$1,802,648.00", "$3,184,604.00",
"$2,351,736.00")
)
setwd("~/Jour772/JOUR628T-dataviz")
mcr_vampiremoney <- read.csv("mcr_reunion_tour_r.csv")
#  group_by(State) |> summarize(Revenue)
#Create data frame
mcr_vampiremoney <- tibble(
State = c("CA", "CA", "CO", "CT", "FL", "GA", "MA", "MI", "MN", "NC",
"NJ", "NV", "NY", "NY", "NY", "NY", "OH", "OK", "OR", "PA",
"TN", "TX", "TX", "TX", "WA"),
Revenue = c("$2,312,402.00", "$9,379,280.00", "$2,303,500.00", "$947,219.00",
"$2,186,088.00", "$1,368,605.00", "$3,875,419.00", "$2,570,993.00",
"$2,323,892.00", "$2,482,991.00", "$3,756,499.00", "$2,671,341.00",
"$1,364,591.00", "$997,090.00", "$1,445,376.00", "$2,955,027.00",
"$1,652,303.00", "$1,963,336.00", "$1,776,165.00", "$2,769,425.00",
"$2,057,097.00", "$2,087,747.00", "$1,802,648.00", "$3,184,604.00",
"$2,351,736.00")
)
knitr::opts_chunk$set(echo = TRUE)
install.packages("treemapify")
install.packages("RColorBrewer")
install.packages('svglite')
library(tidyverse)
library(scales)
library(treemapify)
library(RColorBrewer)
library(svglite)
library(ggplot2)
library(lubridate)
mcr_vampiremoney <- read.csv("mcr_reunion_tour_r.csv")
#  group_by(State) |> summarize(Revenue)
#Create data frame
mcr_vampiremoney <- tibble(
State = c("CA", "CA", "CO", "CT", "FL", "GA", "MA", "MI", "MN", "NC",
"NJ", "NV", "NY", "NY", "NY", "NY", "OH", "OK", "OR", "PA",
"TN", "TX", "TX", "TX", "WA"),
Revenue = c("$2,312,402.00", "$9,379,280.00", "$2,303,500.00", "$947,219.00",
"$2,186,088.00", "$1,368,605.00", "$3,875,419.00", "$2,570,993.00",
"$2,323,892.00", "$2,482,991.00", "$3,756,499.00", "$2,671,341.00",
"$1,364,591.00", "$997,090.00", "$1,445,376.00", "$2,955,027.00",
"$1,652,303.00", "$1,963,336.00", "$1,776,165.00", "$2,769,425.00",
"$2,057,097.00", "$2,087,747.00", "$1,802,648.00", "$3,184,604.00",
"$2,351,736.00")
)
#Clean Revenue column and convert to numeric
mcr_vampiremoney <- mcr_vampiremoney %>%
mutate(Revenue = parse_number(Revenue))
#Sum the revenue for each state
mcr_vmoneysum <- mcr_vampiremoney %>%
group_by(State) %>%
summarise(Total_Revenue = sum(Revenue))
#Print result
print(mcr_vmoneysum)
mcr_vmoneysum %>% ggplot(aes(x = State, y = Total_Revenue)) +
geom_col() +
geom_col(fill="darkred")+
scale_y_continuous(labels = scales::label_number(scale = 1e-06, suffix = "M")) +
scale_x_discrete(guide = guide_axis(angle = 45)) +
labs(title = "'Gimme that vampire money, c'mon!'", subtitle = "After a six-year hiatus, My Chemical Romance's U.S. leg of their 2022 reunion tour \ncollected over $60 million in ticket sales.Here's the breakdown of gross revenue by state.", caption = "Source: Touring Data \n by April Quevedo")+
theme_minimal()+
theme(axis.title = element_blank())
mcr_vampiremoney <- read.csv("mcr_reunion_tour_r.csv")
mcr_area <- mcr_vampiremoney |>
group_by(State) |>
summarize(count_by_state = n())
ggplot(mcr_area, aes(area = count_by_state, fill = State)) +
geom_treemap() +
theme_minimal() +
geom_treemap_text(grow = TRUE, reflow = TRUE,
aes(label = sprintf("%s\n(%s)", State, format(count_by_state, big.mark = ","))),
colour = "white", place = "bottomright", size = 1) +
scale_fill_manual(values = c("CA" = "#060102", "TX" = "#2e0a0f", "NY" = "#861d2c", "FL" = "#491018", "CO" = "#5d141f", "CT" = "#711825", "GA" = "#861d2c", "MA" = "#9a2133", "MI" = "#ae2539", "MN" = "#c22a40", "NC" = "#d23249", "NJ" = "#d6465b", "NV" = "#d95467", "OH" = "#df6e7f", "OK" = "#e48291", "OR" = "#eba4ae", "PA" = "#efb8c0", "TN" = "#f4ccd2", "WA" = "#f6dade" )) +
labs(title = "Black Parade Stops", subtitle = "Stops along the U.S. leg of the MCR Reunion tour", caption = "Source: Touring Data \n by April Quevedo", color = "State")
mcr_vampiremoney <- read.csv("mcr_reunion_tour_r.csv")
mcr_line <- mcr_vampiremoney |>
group_by(M_Y) |>
summarize(count_by_my = n())
ggplot(mcr_line, aes(x=M_Y, y=count_by_my)) +
geom_line() +
geom_point() +
labs(title="Shows over time",
subtitle="MCR's tour kicked off in August and ended in October of 2022.",
caption="Source: Touring Data \n by April Quevedo",
y="# of shows", x="month and year")
